{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f99d3a7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\venkat\n",
      "[nltk_data]     naidu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\venkat\n",
      "[nltk_data]     naidu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import nltk\n",
    "nltk.download('punkt') # Download the Punkt tokenizer models\n",
    "nltk.download('wordnet') # Download the WordNet lexical database\n",
    "from nltk.stem import WordNetLemmatizer # Import the WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer() # Initialize the lemmatizer\n",
    "import pickle # Import pickle for serializing and deserializing Python object structures\n",
    "import numpy as np # Import numpy for array operations\n",
    "\n",
    "from keras.models import load_model # Import load_model from Keras to load the trained model\n",
    "model = load_model('model.h5') # Load the trained model from file\n",
    "import json # Import json for handling JSON data\n",
    "import random # Import random for random selections\n",
    "\n",
    "# Load intents and model data\n",
    "intents = json.loads(open('data.json').read()) # Load the intents JSON file\n",
    "words = pickle.load(open('texts.pkl', 'rb')) # Load the words used in training (vocabulary)\n",
    "classes = pickle.load(open('labels.pkl', 'rb')) # Load the class labels\n",
    "\n",
    "# Function to clean up a sentence (tokenize and lemmatize)\n",
    "def clean_up_sentence(sentence):\n",
    "    sentence_words = nltk.word_tokenize(sentence) # Tokenize the sentence into words\n",
    "    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words] # Lemmatize each word\n",
    "    return sentence_words\n",
    "\n",
    "# Function to convert a sentence into a bag-of-words vector\n",
    "def bow(sentence, words, show_details=True):\n",
    "    sentence_words = clean_up_sentence(sentence) # Clean up the sentence\n",
    "    bag = [0] * len(words) # Initialize the bag of words with 0s\n",
    "    for s in sentence_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == s: \n",
    "                bag[i] = 1 # If the word is in the sentence, mark it as 1 in the bag\n",
    "                if show_details:\n",
    "                    print(f\"found in bag: {w}\")\n",
    "    return np.array(bag)\n",
    "\n",
    "# Function to predict the class of an input sentence\n",
    "def predict_class(sentence, model):\n",
    "    p = bow(sentence, words, show_details=False) # Convert sentence to bag-of-words\n",
    "    res = model.predict(np.array([p]))[0] # Predict the class probabilities\n",
    "    ERROR_THRESHOLD = 0.25 # Set a threshold to filter out weak predictions\n",
    "    results = [[i, r] for i, r in enumerate(res) if r > ERROR_THRESHOLD] # Filter predictions above the threshold\n",
    "    results.sort(key=lambda x: x[1], reverse=True) # Sort by probability in descending order\n",
    "    return_list = [{\"intent\": classes[r[0]], \"probability\": str(r[1])} for r in results] # Create a list of intents and their probabilities\n",
    "    return return_list\n",
    "\n",
    "# Function to get a response based on predicted intent\n",
    "def getResponse(ints, intents_json):\n",
    "    tag = ints[0]['intent'] # Get the intent with the highest probability\n",
    "    for intent in intents_json['intents']:\n",
    "        if intent['tag'] == tag:\n",
    "            return random.choice(intent['responses']) # Return a random response for the matched intent\n",
    "    return \"Sorry, I didn't understand that.\"\n",
    "\n",
    "# Function to generate a chatbot response\n",
    "def chatbot_response(msg):\n",
    "    ints = predict_class(msg, model) # Predict the intent of the message\n",
    "    res = getResponse(ints, intents) # Get the appropriate response\n",
    "    return res\n",
    "\n",
    "# Flask app setup\n",
    "from flask import Flask, render_template, request\n",
    "\n",
    "app = Flask(__name__) # Initialize the Flask app\n",
    "app.static_folder = 'static' # Set the static folder\n",
    "\n",
    "@app.route(\"/\")\n",
    "def home():\n",
    "    return render_template(\"index.html\") # Render the home page\n",
    "\n",
    "@app.route(\"/get\")\n",
    "def get_bot_response():\n",
    "    userText = request.args.get('msg') # Get the user message from the request\n",
    "    return chatbot_response(userText) # Return the chatbot response\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run() # Run the Flask app\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8160ebb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
